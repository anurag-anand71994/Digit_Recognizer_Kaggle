{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digit_kaggle_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anurag-anand71994/Digit_Recognizer_Kaggle/blob/main/digit_kaggle_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DILhUMBrCEu"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WanNdY-3z50I",
        "outputId": "aa1a065e-471a-453e-c634-617796cc72be"
      },
      "source": [
        "path = os.getcwd()\n",
        "data_path = os.path.join(path , \"drive\",\"MyDrive\",\"digit_kaggle\")\n",
        "print(data_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/digit_kaggle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "pB7R4cULnYnP",
        "outputId": "e3f7be0d-f9ac-4ff6-a701-cffe0f5fe049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                       Version\n",
            "----------------------------- --------------\n",
            "absl-py                       0.12.0\n",
            "alabaster                     0.7.12\n",
            "albumentations                0.1.12\n",
            "altair                        4.1.0\n",
            "appdirs                       1.4.4\n",
            "argcomplete                   1.12.3\n",
            "argon2-cffi                   21.1.0\n",
            "arviz                         0.11.4\n",
            "astor                         0.8.1\n",
            "astropy                       4.3.1\n",
            "astunparse                    1.6.3\n",
            "atari-py                      0.2.9\n",
            "atomicwrites                  1.4.0\n",
            "attrs                         21.2.0\n",
            "audioread                     2.1.9\n",
            "autograd                      1.3\n",
            "Babel                         2.9.1\n",
            "backcall                      0.2.0\n",
            "beautifulsoup4                4.6.3\n",
            "bleach                        4.1.0\n",
            "blis                          0.4.1\n",
            "bokeh                         2.3.3\n",
            "Bottleneck                    1.3.2\n",
            "branca                        0.4.2\n",
            "bs4                           0.0.1\n",
            "CacheControl                  0.12.10\n",
            "cached-property               1.5.2\n",
            "cachetools                    4.2.4\n",
            "catalogue                     1.0.0\n",
            "certifi                       2021.10.8\n",
            "cffi                          1.15.0\n",
            "cftime                        1.5.1.1\n",
            "chardet                       3.0.4\n",
            "charset-normalizer            2.0.8\n",
            "click                         7.1.2\n",
            "cloudpickle                   1.3.0\n",
            "cmake                         3.12.0\n",
            "cmdstanpy                     0.9.5\n",
            "colorcet                      2.0.6\n",
            "colorlover                    0.3.0\n",
            "community                     1.0.0b1\n",
            "contextlib2                   0.5.5\n",
            "convertdate                   2.3.2\n",
            "coverage                      3.7.1\n",
            "coveralls                     0.5\n",
            "crcmod                        1.7\n",
            "cufflinks                     0.17.3\n",
            "cupy-cuda111                  9.4.0\n",
            "cvxopt                        1.2.7\n",
            "cvxpy                         1.0.31\n",
            "cycler                        0.11.0\n",
            "cymem                         2.0.6\n",
            "Cython                        0.29.24\n",
            "daft                          0.0.4\n",
            "dask                          2.12.0\n",
            "datascience                   0.10.6\n",
            "debugpy                       1.0.0\n",
            "decorator                     4.4.2\n",
            "defusedxml                    0.7.1\n",
            "descartes                     1.1.0\n",
            "dill                          0.3.4\n",
            "distributed                   1.25.3\n",
            "dlib                          19.18.0\n",
            "dm-tree                       0.1.6\n",
            "docopt                        0.6.2\n",
            "docutils                      0.17.1\n",
            "dopamine-rl                   1.0.5\n",
            "earthengine-api               0.1.290\n",
            "easydict                      1.9\n",
            "ecos                          2.0.7.post1\n",
            "editdistance                  0.5.3\n",
            "en-core-web-sm                2.2.5\n",
            "entrypoints                   0.3\n",
            "ephem                         4.1\n",
            "et-xmlfile                    1.1.0\n",
            "fa2                           0.3.5\n",
            "fastai                        1.0.61\n",
            "fastdtw                       0.3.4\n",
            "fastprogress                  1.0.0\n",
            "fastrlock                     0.8\n",
            "fbprophet                     0.7.1\n",
            "feather-format                0.4.1\n",
            "filelock                      3.4.0\n",
            "firebase-admin                4.4.0\n",
            "fix-yahoo-finance             0.0.22\n",
            "Flask                         1.1.4\n",
            "flatbuffers                   2.0\n",
            "folium                        0.8.3\n",
            "future                        0.16.0\n",
            "gast                          0.4.0\n",
            "GDAL                          2.2.2\n",
            "gdown                         3.6.4\n",
            "gensim                        3.6.0\n",
            "geographiclib                 1.52\n",
            "geopy                         1.17.0\n",
            "gin-config                    0.5.0\n",
            "glob2                         0.7\n",
            "google                        2.0.3\n",
            "google-api-core               1.26.3\n",
            "google-api-python-client      1.12.8\n",
            "google-auth                   1.35.0\n",
            "google-auth-httplib2          0.0.4\n",
            "google-auth-oauthlib          0.4.6\n",
            "google-cloud-bigquery         1.21.0\n",
            "google-cloud-bigquery-storage 1.1.0\n",
            "google-cloud-core             1.0.3\n",
            "google-cloud-datastore        1.8.0\n",
            "google-cloud-firestore        1.7.0\n",
            "google-cloud-language         1.2.0\n",
            "google-cloud-storage          1.18.1\n",
            "google-cloud-translate        1.5.0\n",
            "google-colab                  1.0.0\n",
            "google-pasta                  0.2.0\n",
            "google-resumable-media        0.4.1\n",
            "googleapis-common-protos      1.53.0\n",
            "googledrivedownloader         0.4\n",
            "graphviz                      0.10.1\n",
            "greenlet                      1.1.2\n",
            "grpcio                        1.42.0\n",
            "gspread                       3.0.1\n",
            "gspread-dataframe             3.0.8\n",
            "gym                           0.17.3\n",
            "h5py                          3.1.0\n",
            "HeapDict                      1.0.1\n",
            "hijri-converter               2.2.2\n",
            "holidays                      0.10.5.2\n",
            "holoviews                     1.14.6\n",
            "html5lib                      1.0.1\n",
            "httpimport                    0.5.18\n",
            "httplib2                      0.17.4\n",
            "httplib2shim                  0.0.3\n",
            "humanize                      0.5.1\n",
            "hyperopt                      0.1.2\n",
            "ideep4py                      2.0.0.post3\n",
            "idna                          2.10\n",
            "imageio                       2.4.1\n",
            "imagesize                     1.3.0\n",
            "imbalanced-learn              0.8.1\n",
            "imblearn                      0.0\n",
            "imgaug                        0.2.9\n",
            "importlib-metadata            4.8.2\n",
            "importlib-resources           5.4.0\n",
            "imutils                       0.5.4\n",
            "inflect                       2.1.0\n",
            "iniconfig                     1.1.1\n",
            "intel-openmp                  2021.4.0\n",
            "intervaltree                  2.1.0\n",
            "ipykernel                     4.10.1\n",
            "ipython                       5.5.0\n",
            "ipython-genutils              0.2.0\n",
            "ipython-sql                   0.3.9\n",
            "ipywidgets                    7.6.5\n",
            "itsdangerous                  1.1.0\n",
            "jax                           0.2.25\n",
            "jaxlib                        0.1.71+cuda111\n",
            "jdcal                         1.4.1\n",
            "jedi                          0.18.1\n",
            "jieba                         0.42.1\n",
            "Jinja2                        2.11.3\n",
            "joblib                        1.1.0\n",
            "jpeg4py                       0.1.4\n",
            "jsonschema                    2.6.0\n",
            "jupyter                       1.0.0\n",
            "jupyter-client                5.3.5\n",
            "jupyter-console               5.2.0\n",
            "jupyter-core                  4.9.1\n",
            "jupyterlab-pygments           0.1.2\n",
            "jupyterlab-widgets            1.0.2\n",
            "kaggle                        1.5.12\n",
            "kapre                         0.3.6\n",
            "keras                         2.7.0\n",
            "Keras-Preprocessing           1.1.2\n",
            "keras-vis                     0.4.1\n",
            "kiwisolver                    1.3.2\n",
            "korean-lunar-calendar         0.2.1\n",
            "libclang                      12.0.0\n",
            "librosa                       0.8.1\n",
            "lightgbm                      2.2.3\n",
            "llvmlite                      0.34.0\n",
            "lmdb                          0.99\n",
            "LunarCalendar                 0.0.9\n",
            "lxml                          4.2.6\n",
            "Markdown                      3.3.6\n",
            "MarkupSafe                    2.0.1\n",
            "matplotlib                    3.2.2\n",
            "matplotlib-inline             0.1.3\n",
            "matplotlib-venn               0.11.6\n",
            "missingno                     0.5.0\n",
            "mistune                       0.8.4\n",
            "mizani                        0.6.0\n",
            "mkl                           2019.0\n",
            "mlxtend                       0.14.0\n",
            "more-itertools                8.12.0\n",
            "moviepy                       0.2.3.5\n",
            "mpmath                        1.2.1\n",
            "msgpack                       1.0.3\n",
            "multiprocess                  0.70.12.2\n",
            "multitasking                  0.0.10\n",
            "murmurhash                    1.0.6\n",
            "music21                       5.5.0\n",
            "natsort                       5.5.0\n",
            "nbclient                      0.5.9\n",
            "nbconvert                     5.6.1\n",
            "nbformat                      5.1.3\n",
            "nest-asyncio                  1.5.4\n",
            "netCDF4                       1.5.8\n",
            "networkx                      2.6.3\n",
            "nibabel                       3.0.2\n",
            "nltk                          3.2.5\n",
            "notebook                      5.3.1\n",
            "numba                         0.51.2\n",
            "numexpr                       2.7.3\n",
            "numpy                         1.19.5\n",
            "nvidia-ml-py3                 7.352.0\n",
            "oauth2client                  4.1.3\n",
            "oauthlib                      3.1.1\n",
            "okgrade                       0.4.3\n",
            "opencv-contrib-python         4.1.2.30\n",
            "opencv-python                 4.1.2.30\n",
            "openpyxl                      2.5.9\n",
            "opt-einsum                    3.3.0\n",
            "osqp                          0.6.2.post0\n",
            "packaging                     21.3\n",
            "palettable                    3.3.0\n",
            "pandas                        1.1.5\n",
            "pandas-datareader             0.9.0\n",
            "pandas-gbq                    0.13.3\n",
            "pandas-profiling              1.4.1\n",
            "pandocfilters                 1.5.0\n",
            "panel                         0.12.1\n",
            "param                         1.12.0\n",
            "parso                         0.8.3\n",
            "pathlib                       1.0.1\n",
            "patsy                         0.5.2\n",
            "pep517                        0.12.0\n",
            "pexpect                       4.8.0\n",
            "pickleshare                   0.7.5\n",
            "Pillow                        7.1.2\n",
            "pip                           21.1.3\n",
            "pip-tools                     6.2.0\n",
            "plac                          1.1.3\n",
            "plotly                        4.4.1\n",
            "plotnine                      0.6.0\n",
            "pluggy                        0.7.1\n",
            "pooch                         1.5.2\n",
            "portpicker                    1.3.9\n",
            "prefetch-generator            1.0.1\n",
            "preshed                       3.0.6\n",
            "prettytable                   2.4.0\n",
            "progressbar2                  3.38.0\n",
            "prometheus-client             0.12.0\n",
            "promise                       2.3\n",
            "prompt-toolkit                1.0.18\n",
            "protobuf                      3.17.3\n",
            "psutil                        5.4.8\n",
            "psycopg2                      2.7.6.1\n",
            "ptyprocess                    0.7.0\n",
            "py                            1.11.0\n",
            "pyarrow                       3.0.0\n",
            "pyasn1                        0.4.8\n",
            "pyasn1-modules                0.2.8\n",
            "pycocotools                   2.0.3\n",
            "pycparser                     2.21\n",
            "pyct                          0.4.8\n",
            "pydata-google-auth            1.2.0\n",
            "pydot                         1.3.0\n",
            "pydot-ng                      2.0.0\n",
            "pydotplus                     2.0.2\n",
            "PyDrive                       1.3.1\n",
            "pyemd                         0.5.1\n",
            "pyerfa                        2.0.0.1\n",
            "pyglet                        1.5.0\n",
            "Pygments                      2.6.1\n",
            "pygobject                     3.26.1\n",
            "pymc3                         3.11.4\n",
            "PyMeeus                       0.5.11\n",
            "pymongo                       3.12.1\n",
            "pymystem3                     0.2.0\n",
            "PyOpenGL                      3.1.5\n",
            "pyparsing                     3.0.6\n",
            "pyrsistent                    0.18.0\n",
            "pysndfile                     1.3.8\n",
            "PySocks                       1.7.1\n",
            "pystan                        2.19.1.1\n",
            "pytest                        3.6.4\n",
            "python-apt                    0.0.0\n",
            "python-chess                  0.23.11\n",
            "python-dateutil               2.8.2\n",
            "python-louvain                0.15\n",
            "python-slugify                5.0.2\n",
            "python-utils                  2.5.6\n",
            "pytz                          2018.9\n",
            "pyviz-comms                   2.1.0\n",
            "PyWavelets                    1.2.0\n",
            "PyYAML                        3.13\n",
            "pyzmq                         22.3.0\n",
            "qdldl                         0.1.5.post0\n",
            "qtconsole                     5.2.1\n",
            "QtPy                          1.11.2\n",
            "regex                         2019.12.20\n",
            "requests                      2.23.0\n",
            "requests-oauthlib             1.3.0\n",
            "resampy                       0.2.2\n",
            "retrying                      1.3.3\n",
            "rpy2                          3.4.5\n",
            "rsa                           4.8\n",
            "scikit-image                  0.18.3\n",
            "scikit-learn                  1.0.1\n",
            "scipy                         1.4.1\n",
            "screen-resolution-extra       0.0.0\n",
            "scs                           2.1.4\n",
            "seaborn                       0.11.2\n",
            "semver                        2.13.0\n",
            "Send2Trash                    1.8.0\n",
            "setuptools                    57.4.0\n",
            "setuptools-git                1.2\n",
            "Shapely                       1.8.0\n",
            "simplegeneric                 0.8.1\n",
            "six                           1.15.0\n",
            "sklearn                       0.0\n",
            "sklearn-pandas                1.8.0\n",
            "smart-open                    5.2.1\n",
            "snowballstemmer               2.2.0\n",
            "sortedcontainers              2.4.0\n",
            "SoundFile                     0.10.3.post1\n",
            "spacy                         2.2.4\n",
            "Sphinx                        1.8.6\n",
            "sphinxcontrib-serializinghtml 1.1.5\n",
            "sphinxcontrib-websupport      1.2.4\n",
            "SQLAlchemy                    1.4.27\n",
            "sqlparse                      0.4.2\n",
            "srsly                         1.0.5\n",
            "statsmodels                   0.10.2\n",
            "sympy                         1.7.1\n",
            "tables                        3.4.4\n",
            "tabulate                      0.8.9\n",
            "tblib                         1.7.0\n",
            "tensorboard                   2.7.0\n",
            "tensorboard-data-server       0.6.1\n",
            "tensorboard-plugin-wit        1.8.0\n",
            "tensorflow                    2.7.0\n",
            "tensorflow-datasets           4.0.1\n",
            "tensorflow-estimator          2.7.0\n",
            "tensorflow-gcs-config         2.7.0\n",
            "tensorflow-hub                0.12.0\n",
            "tensorflow-io-gcs-filesystem  0.22.0\n",
            "tensorflow-metadata           1.4.0\n",
            "tensorflow-probability        0.15.0\n",
            "termcolor                     1.1.0\n",
            "terminado                     0.12.1\n",
            "testpath                      0.5.0\n",
            "text-unidecode                1.3\n",
            "textblob                      0.15.3\n",
            "Theano-PyMC                   1.1.2\n",
            "thinc                         7.4.0\n",
            "threadpoolctl                 3.0.0\n",
            "tifffile                      2021.11.2\n",
            "toml                          0.10.2\n",
            "tomli                         1.2.2\n",
            "toolz                         0.11.2\n",
            "torch                         1.10.0+cu111\n",
            "torchaudio                    0.10.0+cu111\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.11.0\n",
            "torchvision                   0.11.1+cu111\n",
            "tornado                       5.1.1\n",
            "tqdm                          4.62.3\n",
            "traitlets                     5.1.1\n",
            "tweepy                        3.10.0\n",
            "typeguard                     2.7.1\n",
            "typing-extensions             3.10.0.2\n",
            "tzlocal                       1.5.1\n",
            "uritemplate                   3.0.1\n",
            "urllib3                       1.24.3\n",
            "vega-datasets                 0.9.0\n",
            "wasabi                        0.8.2\n",
            "wcwidth                       0.2.5\n",
            "webencodings                  0.5.1\n",
            "Werkzeug                      1.0.1\n",
            "wheel                         0.37.0\n",
            "widgetsnbextension            3.5.2\n",
            "wordcloud                     1.5.0\n",
            "wrapt                         1.13.3\n",
            "xarray                        0.18.2\n",
            "xgboost                       0.90\n",
            "xkit                          0.0.0\n",
            "xlrd                          1.1.0\n",
            "xlwt                          1.3.0\n",
            "yellowbrick                   1.3.post1\n",
            "zict                          2.0.0\n",
            "zipp                          3.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yFvLcycY2vJ7",
        "outputId": "39a9eac3-d46c-43d5-973b-305816410048"
      },
      "source": [
        "\"\"\"The below command is to be used to unzip the training and testing data zip\"\"\"\n",
        "#!unzip /content/drive/MyDrive/digit_kaggle/digit-recognizer.zip  -d /content/drive/MyDrive/digit_kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The below command is to be used to unzip the training and testing data zip'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOUQbP3A4HTv"
      },
      "source": [
        "train = pd.read_csv(r\"/content/drive/MyDrive/digit_kaggle/\" + \"train.csv\")\n",
        "test = pd.read_csv(r\"/content/drive/MyDrive/digit_kaggle/\" + \"test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrgLqcB14h-Y",
        "outputId": "281f5f8b-065f-4cb1-f047-e081a82c6a48"
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42000, 785)\n",
            "(28000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "3S42tfXY5zBe",
        "outputId": "34fdd732-7768-46fc-f118-2f3d42649511"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel744</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.00000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.00000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.00000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.456643</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00300</td>\n",
              "      <td>0.011190</td>\n",
              "      <td>0.005143</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.001310</td>\n",
              "      <td>0.010548</td>\n",
              "      <td>0.027262</td>\n",
              "      <td>0.050905</td>\n",
              "      <td>0.066405</td>\n",
              "      <td>0.129571</td>\n",
              "      <td>...</td>\n",
              "      <td>3.772524</td>\n",
              "      <td>2.748905</td>\n",
              "      <td>1.796452</td>\n",
              "      <td>1.089905</td>\n",
              "      <td>0.563190</td>\n",
              "      <td>0.239571</td>\n",
              "      <td>0.093524</td>\n",
              "      <td>0.024833</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>0.001405</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006143</td>\n",
              "      <td>0.035833</td>\n",
              "      <td>0.082357</td>\n",
              "      <td>0.114905</td>\n",
              "      <td>0.178714</td>\n",
              "      <td>0.301452</td>\n",
              "      <td>0.413643</td>\n",
              "      <td>0.513667</td>\n",
              "      <td>0.558833</td>\n",
              "      <td>0.677857</td>\n",
              "      <td>0.60281</td>\n",
              "      <td>0.489238</td>\n",
              "      <td>0.340214</td>\n",
              "      <td>0.219286</td>\n",
              "      <td>0.117095</td>\n",
              "      <td>0.059024</td>\n",
              "      <td>0.02019</td>\n",
              "      <td>0.017238</td>\n",
              "      <td>0.002857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.887730</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.56812</td>\n",
              "      <td>1.626927</td>\n",
              "      <td>1.053972</td>\n",
              "      <td>0.043916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.078072</td>\n",
              "      <td>0.232634</td>\n",
              "      <td>1.131661</td>\n",
              "      <td>2.310396</td>\n",
              "      <td>3.121847</td>\n",
              "      <td>3.259128</td>\n",
              "      <td>4.992894</td>\n",
              "      <td>...</td>\n",
              "      <td>26.957829</td>\n",
              "      <td>22.879248</td>\n",
              "      <td>18.595109</td>\n",
              "      <td>14.434439</td>\n",
              "      <td>10.517823</td>\n",
              "      <td>6.469315</td>\n",
              "      <td>3.976306</td>\n",
              "      <td>1.846016</td>\n",
              "      <td>0.139556</td>\n",
              "      <td>0.287891</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.949803</td>\n",
              "      <td>2.350859</td>\n",
              "      <td>3.934280</td>\n",
              "      <td>4.543583</td>\n",
              "      <td>5.856772</td>\n",
              "      <td>7.219742</td>\n",
              "      <td>8.928286</td>\n",
              "      <td>10.004069</td>\n",
              "      <td>10.129595</td>\n",
              "      <td>11.254931</td>\n",
              "      <td>10.69603</td>\n",
              "      <td>9.480066</td>\n",
              "      <td>7.950251</td>\n",
              "      <td>6.312890</td>\n",
              "      <td>4.633819</td>\n",
              "      <td>3.274488</td>\n",
              "      <td>1.75987</td>\n",
              "      <td>1.894498</td>\n",
              "      <td>0.414264</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>116.00000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>216.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>157.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>243.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>177.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.00000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>253.00000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              label   pixel0   pixel1  ...  pixel781  pixel782  pixel783\n",
              "count  42000.000000  42000.0  42000.0  ...   42000.0   42000.0   42000.0\n",
              "mean       4.456643      0.0      0.0  ...       0.0       0.0       0.0\n",
              "std        2.887730      0.0      0.0  ...       0.0       0.0       0.0\n",
              "min        0.000000      0.0      0.0  ...       0.0       0.0       0.0\n",
              "25%        2.000000      0.0      0.0  ...       0.0       0.0       0.0\n",
              "50%        4.000000      0.0      0.0  ...       0.0       0.0       0.0\n",
              "75%        7.000000      0.0      0.0  ...       0.0       0.0       0.0\n",
              "max        9.000000      0.0      0.0  ...       0.0       0.0       0.0\n",
              "\n",
              "[8 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "e8YwK1Zl54fz",
        "outputId": "f95372ef-2a49-464a-bd61-10a87b0ec66b"
      },
      "source": [
        "test.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>pixel39</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel744</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.000000</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>28000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001357</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.016786</td>\n",
              "      <td>0.031714</td>\n",
              "      <td>0.056000</td>\n",
              "      <td>0.100464</td>\n",
              "      <td>0.166929</td>\n",
              "      <td>...</td>\n",
              "      <td>3.272536</td>\n",
              "      <td>2.371464</td>\n",
              "      <td>1.454357</td>\n",
              "      <td>0.846286</td>\n",
              "      <td>0.509750</td>\n",
              "      <td>0.254750</td>\n",
              "      <td>0.062107</td>\n",
              "      <td>0.015250</td>\n",
              "      <td>0.000786</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005429</td>\n",
              "      <td>0.024179</td>\n",
              "      <td>0.036250</td>\n",
              "      <td>0.083143</td>\n",
              "      <td>0.134107</td>\n",
              "      <td>0.201071</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.366714</td>\n",
              "      <td>0.468143</td>\n",
              "      <td>0.589429</td>\n",
              "      <td>0.656964</td>\n",
              "      <td>0.569714</td>\n",
              "      <td>0.464214</td>\n",
              "      <td>0.323679</td>\n",
              "      <td>0.164607</td>\n",
              "      <td>0.073214</td>\n",
              "      <td>0.028036</td>\n",
              "      <td>0.011250</td>\n",
              "      <td>0.006536</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.227093</td>\n",
              "      <td>1.566275</td>\n",
              "      <td>1.513515</td>\n",
              "      <td>2.674449</td>\n",
              "      <td>3.216234</td>\n",
              "      <td>4.549478</td>\n",
              "      <td>5.470524</td>\n",
              "      <td>...</td>\n",
              "      <td>25.211706</td>\n",
              "      <td>21.240003</td>\n",
              "      <td>16.643468</td>\n",
              "      <td>12.637953</td>\n",
              "      <td>9.963879</td>\n",
              "      <td>7.031504</td>\n",
              "      <td>3.040514</td>\n",
              "      <td>1.265562</td>\n",
              "      <td>0.131475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.640468</td>\n",
              "      <td>2.234963</td>\n",
              "      <td>2.493982</td>\n",
              "      <td>3.777711</td>\n",
              "      <td>4.946940</td>\n",
              "      <td>6.262819</td>\n",
              "      <td>7.714814</td>\n",
              "      <td>8.243535</td>\n",
              "      <td>8.974038</td>\n",
              "      <td>10.488695</td>\n",
              "      <td>11.209508</td>\n",
              "      <td>10.204173</td>\n",
              "      <td>9.402197</td>\n",
              "      <td>7.878854</td>\n",
              "      <td>5.473293</td>\n",
              "      <td>3.616811</td>\n",
              "      <td>1.813602</td>\n",
              "      <td>1.205211</td>\n",
              "      <td>0.807475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>236.000000</td>\n",
              "      <td>163.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>135.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>252.000000</td>\n",
              "      <td>245.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>193.000000</td>\n",
              "      <td>187.000000</td>\n",
              "      <td>119.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        pixel0   pixel1   pixel2  ...  pixel781  pixel782  pixel783\n",
              "count  28000.0  28000.0  28000.0  ...   28000.0   28000.0   28000.0\n",
              "mean       0.0      0.0      0.0  ...       0.0       0.0       0.0\n",
              "std        0.0      0.0      0.0  ...       0.0       0.0       0.0\n",
              "min        0.0      0.0      0.0  ...       0.0       0.0       0.0\n",
              "25%        0.0      0.0      0.0  ...       0.0       0.0       0.0\n",
              "50%        0.0      0.0      0.0  ...       0.0       0.0       0.0\n",
              "75%        0.0      0.0      0.0  ...       0.0       0.0       0.0\n",
              "max        0.0      0.0      0.0  ...       0.0       0.0       0.0\n",
              "\n",
              "[8 rows x 784 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "yq8KoFXY6WWD",
        "outputId": "885cc47d-1c56-4850-d1d1-f51b427917e9"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(train['label'])\n",
        "plt.tight_layout();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6832fa627a04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2ZfsHdA7EfR"
      },
      "source": [
        "# Extract features\n",
        "features = train.drop('label', axis=1)\n",
        "\n",
        "# Extract label\n",
        "y_train = train['label']\n",
        "\n",
        "# Train images\n",
        "X_ = np.array(features)\n",
        "X_train = X_.reshape(X_.shape[0], 28, 28)\n",
        "\n",
        "# Test images\n",
        "X_test = np.array(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "jHU29PQ_7Nv1",
        "outputId": "998befa5-3565-434b-be46-e590bf89bfaf"
      },
      "source": [
        "plt.imshow(X_train[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33nAfIvz7Wca",
        "outputId": "dc58260c-0213-40c0-9cc3-25415fd354e8"
      },
      "source": [
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42000, 28, 28) (28000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LK2w2A370GW",
        "outputId": "30d1ce8b-e057-4727-abd3-505d1692a00b"
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
        "print(X_train.shape , X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42000, 28, 28, 1) (28000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1laljQS8LPU"
      },
      "source": [
        "X_train = X_train/255.\n",
        "X_test = X_test/255.\n",
        "\n",
        "X_train_val = X_train[38000 : ]\n",
        "y_train_val = y_train[38000 : ]\n",
        "\n",
        "X_train = X_train[:38000]\n",
        "y_train = y_train[:38000]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4VSd1xTNw3g",
        "outputId": "2a19f939-ae1a-4497-81f1-0fce30f409ab"
      },
      "source": [
        "# 1. Create the model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                             tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "                             tf.keras.layers.Dense(10) #10 is the number of classes\n",
        "])\n",
        "\n",
        "#2 Compile the model\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1188/1188 [==============================] - 4s 3ms/step - loss: 0.3027 - accuracy: 0.9128\n",
            "Epoch 2/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.1316 - accuracy: 0.9621\n",
            "Epoch 3/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0894 - accuracy: 0.9736\n",
            "Epoch 4/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0673 - accuracy: 0.9795\n",
            "Epoch 5/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0513 - accuracy: 0.9844\n",
            "Epoch 6/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0390 - accuracy: 0.9888\n",
            "Epoch 7/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0311 - accuracy: 0.9909\n",
            "Epoch 8/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0245 - accuracy: 0.9929\n",
            "Epoch 9/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0207 - accuracy: 0.9938\n",
            "Epoch 10/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0149 - accuracy: 0.9955\n",
            "Epoch 11/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0141 - accuracy: 0.9962\n",
            "Epoch 12/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0102 - accuracy: 0.9975\n",
            "Epoch 13/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0107 - accuracy: 0.9969\n",
            "Epoch 14/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0072 - accuracy: 0.9981\n",
            "Epoch 15/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0080 - accuracy: 0.9976\n",
            "Epoch 16/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0046 - accuracy: 0.9988\n",
            "Epoch 17/50\n",
            "1188/1188 [==============================] - 4s 3ms/step - loss: 0.0121 - accuracy: 0.9957\n",
            "Epoch 18/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 19/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0050 - accuracy: 0.9984\n",
            "Epoch 20/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0045 - accuracy: 0.9985\n",
            "Epoch 21/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0047 - accuracy: 0.9985\n",
            "Epoch 22/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0065 - accuracy: 0.9979\n",
            "Epoch 23/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0024 - accuracy: 0.9996\n",
            "Epoch 24/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0054 - accuracy: 0.9983\n",
            "Epoch 25/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0032 - accuracy: 0.9992\n",
            "Epoch 26/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0045 - accuracy: 0.9987\n",
            "Epoch 27/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 6.7773e-04 - accuracy: 0.9998\n",
            "Epoch 28/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0072 - accuracy: 0.9976\n",
            "Epoch 29/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0020 - accuracy: 0.9994\n",
            "Epoch 30/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0014 - accuracy: 0.9997\n",
            "Epoch 31/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0044 - accuracy: 0.9986\n",
            "Epoch 32/50\n",
            "1188/1188 [==============================] - 4s 3ms/step - loss: 0.0036 - accuracy: 0.9987\n",
            "Epoch 33/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0033 - accuracy: 0.9991\n",
            "Epoch 34/50\n",
            "1188/1188 [==============================] - 4s 3ms/step - loss: 0.0043 - accuracy: 0.9988\n",
            "Epoch 35/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0014 - accuracy: 0.9996\n",
            "Epoch 36/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0020 - accuracy: 0.9993\n",
            "Epoch 37/50\n",
            "1188/1188 [==============================] - 4s 3ms/step - loss: 0.0043 - accuracy: 0.9984\n",
            "Epoch 38/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0034 - accuracy: 0.9988\n",
            "Epoch 39/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 7.5745e-04 - accuracy: 0.9998\n",
            "Epoch 40/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 1.6522e-04 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 6.1670e-05 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 3.3671e-05 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0055 - accuracy: 0.9984\n",
            "Epoch 44/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0042 - accuracy: 0.9987\n",
            "Epoch 45/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0015 - accuracy: 0.9996\n",
            "Epoch 46/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0033 - accuracy: 0.9988\n",
            "Epoch 47/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0026 - accuracy: 0.9993\n",
            "Epoch 48/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 8.9116e-04 - accuracy: 0.9999\n",
            "Epoch 49/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0011 - accuracy: 0.9996\n",
            "Epoch 50/50\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0059 - accuracy: 0.9982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "YcX5abrYN5ff",
        "outputId": "4e4ee5c1-19b4-42b6-be5f-94d4aaf2aff4"
      },
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(12,7))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5b93a7f690>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q98cIetWR8D6"
      },
      "source": [
        "predictions = np.argmax(model.predict(X_train_val), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Undof3YeSCGw",
        "outputId": "d59099af-60c1-41f5-fa49-cfe47753c1d7"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 7, 5, ..., 7, 6, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8_SyFFnQwnr",
        "outputId": "4c11f59c-dd7c-450a-87f4-f4ed21eedee3"
      },
      "source": [
        "print(metrics.classification_report(y_train_val, predictions, digits=10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0  0.9750000000 0.9976744186 0.9862068966       430\n",
            "           1  0.9711751663 0.9909502262 0.9809630459       442\n",
            "           2  0.9807692308 0.9596774194 0.9701086957       372\n",
            "           3  0.9854721550 0.9667458432 0.9760191847       421\n",
            "           4  0.9809523810 0.9763033175 0.9786223278       422\n",
            "           5  0.9846153846 0.9580838323 0.9711684370       334\n",
            "           6  0.9636803874 0.9950000000 0.9790897909       400\n",
            "           7  0.9800995025 0.9825436409 0.9813200498       401\n",
            "           8  0.9673024523 0.9366754617 0.9517426273       379\n",
            "           9  0.9629629630 0.9774436090 0.9701492537       399\n",
            "\n",
            "    accuracy                      0.9750000000      4000\n",
            "   macro avg  0.9752029623 0.9741097769 0.9745390309      4000\n",
            "weighted avg  0.9750961762 0.9750000000 0.9749333220      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aflPgVR5Slgk"
      },
      "source": [
        "datagen = ImageDataGenerator(rotation_range=20,\n",
        "                  width_shift_range=0.20,\n",
        "                  shear_range=15,\n",
        "                  zoom_range=0.10,\n",
        "                  validation_split=0,\n",
        "                  horizontal_flip=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgIBodTE1FpD"
      },
      "source": [
        "datagen.fit(X_train)\n",
        "generator_train  = datagen.flow(X_train,y_train,batch_size=256)\n",
        "#generator_validation  = datagen.flow(X,y,batch_size=64,subset='validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3k9uAFP2orr",
        "outputId": "17c71f0c-6bd6-42c7-fed0-ff432df13ed9"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                             tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "                             tf.keras.layers.Dense(10) #10 is the number of classes\n",
        "])\n",
        "\n",
        "#2 Compile the model\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "\n",
        "history = model.fit(generator_train.x, generator_train.y, epochs=100,)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1188/1188 [==============================] - 4s 3ms/step - loss: 0.2991 - accuracy: 0.9162\n",
            "Epoch 2/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.1360 - accuracy: 0.9607\n",
            "Epoch 3/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0910 - accuracy: 0.9733\n",
            "Epoch 4/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0687 - accuracy: 0.9793\n",
            "Epoch 5/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0514 - accuracy: 0.9847\n",
            "Epoch 6/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0401 - accuracy: 0.9881\n",
            "Epoch 7/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0311 - accuracy: 0.9908\n",
            "Epoch 8/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0244 - accuracy: 0.9925\n",
            "Epoch 9/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0191 - accuracy: 0.9949\n",
            "Epoch 10/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0169 - accuracy: 0.9954\n",
            "Epoch 11/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0115 - accuracy: 0.9967\n",
            "Epoch 12/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0113 - accuracy: 0.9969\n",
            "Epoch 13/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0088 - accuracy: 0.9978\n",
            "Epoch 14/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0065 - accuracy: 0.9982\n",
            "Epoch 15/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0095 - accuracy: 0.9971\n",
            "Epoch 16/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0043 - accuracy: 0.9989\n",
            "Epoch 17/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0053 - accuracy: 0.9987\n",
            "Epoch 18/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0052 - accuracy: 0.9987\n",
            "Epoch 19/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0063 - accuracy: 0.9981\n",
            "Epoch 20/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0036 - accuracy: 0.9989\n",
            "Epoch 21/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0066 - accuracy: 0.9980\n",
            "Epoch 22/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0029 - accuracy: 0.9991\n",
            "Epoch 23/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0051 - accuracy: 0.9984\n",
            "Epoch 24/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 25/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0042 - accuracy: 0.9986\n",
            "Epoch 26/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0035 - accuracy: 0.9991\n",
            "Epoch 27/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0028 - accuracy: 0.9993\n",
            "Epoch 28/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 2.7035e-04 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 1.2705e-04 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 9.0079e-05 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0032 - accuracy: 0.9992\n",
            "Epoch 32/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0109 - accuracy: 0.9967\n",
            "Epoch 33/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 4.0603e-04 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 1.8787e-04 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 8.3085e-05 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 5.9615e-05 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0065 - accuracy: 0.9983\n",
            "Epoch 38/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0062 - accuracy: 0.9977\n",
            "Epoch 39/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0013 - accuracy: 0.9997\n",
            "Epoch 40/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0027 - accuracy: 0.9991\n",
            "Epoch 41/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 5.9243e-04 - accuracy: 0.9998\n",
            "Epoch 42/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0063 - accuracy: 0.9979\n",
            "Epoch 43/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0037 - accuracy: 0.9987\n",
            "Epoch 44/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0022 - accuracy: 0.9992\n",
            "Epoch 45/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0023 - accuracy: 0.9993\n",
            "Epoch 46/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0025 - accuracy: 0.9992\n",
            "Epoch 47/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0028 - accuracy: 0.9990\n",
            "Epoch 48/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0030 - accuracy: 0.9990\n",
            "Epoch 49/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 2.0567e-04 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0031 - accuracy: 0.9991\n",
            "Epoch 51/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0039 - accuracy: 0.9987\n",
            "Epoch 52/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0013 - accuracy: 0.9997\n",
            "Epoch 53/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 1.6834e-04 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 3.1410e-05 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 1.9045e-05 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 1.3876e-05 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 1.0649e-05 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 7.7886e-06 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0042 - accuracy: 0.9989\n",
            "Epoch 60/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0074 - accuracy: 0.9977\n",
            "Epoch 61/100\n",
            "1188/1188 [==============================] - 4s 3ms/step - loss: 0.0012 - accuracy: 0.9996\n",
            "Epoch 62/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0014 - accuracy: 0.9994\n",
            "Epoch 63/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0029 - accuracy: 0.9994\n",
            "Epoch 64/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 1.4580e-04 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 2.0974e-05 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 1.2465e-05 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 9.2597e-06 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 6.9035e-06 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 5.1740e-06 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 4.0991e-06 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 3.1212e-06 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 2.6569e-06 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0085 - accuracy: 0.9977\n",
            "Epoch 74/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0013 - accuracy: 0.9996\n",
            "Epoch 75/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0033 - accuracy: 0.9989\n",
            "Epoch 76/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0016 - accuracy: 0.9997\n",
            "Epoch 77/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 3.9129e-05 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 1.2119e-05 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 7.8972e-06 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 5.6611e-06 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 4.1155e-06 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 3.0813e-06 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1188/1188 [==============================] - 4s 3ms/step - loss: 2.2026e-06 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 1.5807e-06 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "1188/1188 [==============================] - 4s 3ms/step - loss: 1.1543e-06 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0047 - accuracy: 0.9986\n",
            "Epoch 87/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0026 - accuracy: 0.9992\n",
            "Epoch 88/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0013 - accuracy: 0.9995\n",
            "Epoch 89/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0051 - accuracy: 0.9984\n",
            "Epoch 90/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 8.5044e-04 - accuracy: 0.9997\n",
            "Epoch 91/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 4.3919e-04 - accuracy: 0.9999\n",
            "Epoch 92/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0044 - accuracy: 0.9987\n",
            "Epoch 93/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0016 - accuracy: 0.9996\n",
            "Epoch 94/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0020 - accuracy: 0.9994\n",
            "Epoch 95/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 0.0034 - accuracy: 0.9993\n",
            "Epoch 96/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 2.5621e-04 - accuracy: 0.9999\n",
            "Epoch 97/100\n",
            "1188/1188 [==============================] - 4s 3ms/step - loss: 1.9633e-05 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 6.9929e-06 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1188/1188 [==============================] - 3s 3ms/step - loss: 4.9183e-06 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "1188/1188 [==============================] - 4s 3ms/step - loss: 3.5265e-06 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "7zs6CtRd3ifp",
        "outputId": "2343ba57-8e04-4646-e7a5-aea247a76b58"
      },
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(12,7))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2178c2de90>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnVqliVj3mqS",
        "outputId": "991997be-725e-45d5-d876-fb4f90445c9f"
      },
      "source": [
        "predictions = np.argmax(model.predict(X_train_val), axis=1)\n",
        "print(metrics.classification_report(y_train_val, predictions, digits=10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0  0.9839080460 0.9953488372 0.9895953757       430\n",
            "           1  0.9842696629 0.9909502262 0.9875986471       442\n",
            "           2  0.9809264305 0.9677419355 0.9742895805       372\n",
            "           3  0.9878345499 0.9643705463 0.9759615385       421\n",
            "           4  0.9764150943 0.9810426540 0.9787234043       422\n",
            "           5  0.9817073171 0.9640718563 0.9728096677       334\n",
            "           6  0.9850374065 0.9875000000 0.9862671660       400\n",
            "           7  0.9851485149 0.9925187032 0.9888198758       401\n",
            "           8  0.9507772021 0.9683377309 0.9594771242       379\n",
            "           9  0.9774436090 0.9774436090 0.9774436090       399\n",
            "\n",
            "    accuracy                      0.9795000000      4000\n",
            "   macro avg  0.9793467833 0.9789326099 0.9790985989      4000\n",
            "weighted avg  0.9795630297 0.9795000000 0.9794913294      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yU9g5Dm18vy"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpumREOn2BEB"
      },
      "source": [
        "input_shape = (28,28,1)\n",
        "input_layer = Input(input_shape)\n",
        "layer = Conv2D(32,(5,5),activation = tf.nn.relu, padding='same', input_shape = input_shape)(input_layer)\n",
        "layer = MaxPool2D((2,2))(layer)\n",
        "layer = Conv2D(64,(3,3),activation = tf.nn.relu, padding='same')(layer)\n",
        "layer = Conv2D(64,(3,3),activation = tf.nn.relu, padding='same')(layer)\n",
        "layer = MaxPool2D((2,2))(layer)\n",
        "layer = Conv2D(128,(3,3),activation = tf.nn.relu, padding='same')(layer)\n",
        "layer = Conv2D(128,(3,3),activation = tf.nn.relu, padding='same')(layer)\n",
        "layer = Conv2D(128,(3,3),activation = tf.nn.relu, padding='same')(layer)\n",
        "layer = MaxPool2D((2,2))(layer)\n",
        "\n",
        "\n",
        "\n",
        "flatten = Flatten()(layer)\n",
        "\n",
        "layer = Dense(512,activation = tf.nn.relu)(flatten)\n",
        "layer = Dropout(0.25)(layer)\n",
        "layer = Dense(512,activation = tf.nn.relu)(layer)\n",
        "layer  = Dropout(0.25)(layer)\n",
        "layer = Dense(512,activation = tf.nn.relu)(layer)\n",
        "layer  = Dropout(0.25)(layer)\n",
        "output_layer = Dense(10, activation = tf.nn.softmax)(layer)\n",
        "model = Model(input_layer,output_layer)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,optimizer='Adam', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSA6iedAENSw",
        "outputId": "d02a47c9-fd3e-435a-c9c6-167f41ea7d17"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 512)               590336    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,546,058\n",
            "Trainable params: 1,546,058\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x1pW8nIm2aoN",
        "outputId": "b237fb4f-e77c-4590-d151-a4ef2e4f47f1"
      },
      "source": [
        "history = model.fit(generator_train.x, generator_train.y, epochs=100, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3324 - accuracy: 0.1267\n",
            "Epoch 2/100\n",
            "2375/2375 [==============================] - 20s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 3/100\n",
            "2375/2375 [==============================] - 20s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 4/100\n",
            "2375/2375 [==============================] - 20s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 5/100\n",
            "2375/2375 [==============================] - 20s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 6/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 7/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 8/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 9/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 10/100\n",
            "2375/2375 [==============================] - 20s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 11/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 12/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 13/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 14/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 15/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 16/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 17/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 18/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 19/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 20/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 21/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 22/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 23/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 24/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3577 - accuracy: 0.1034\n",
            "Epoch 25/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 26/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 27/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 28/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 29/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 30/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 31/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 32/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 33/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 34/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 35/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 36/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 37/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 38/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 39/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 40/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 41/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 42/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 43/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 44/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 45/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 46/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 47/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 48/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 49/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 50/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 51/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 52/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 53/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 54/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 55/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 56/100\n",
            "2375/2375 [==============================] - 21s 9ms/step - loss: 2.3495 - accuracy: 0.1116\n",
            "Epoch 57/100\n",
            "1909/2375 [=======================>......] - ETA: 4s - loss: 2.3504 - accuracy: 0.1108"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-76b1f0c84bae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \"\"\"\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    510\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjZj2tc72Ri6",
        "outputId": "ca281b94-b50d-4a96-a3c7-4ec67abf6b35"
      },
      "source": [
        "predictions = np.argmax(model.predict(X_train_val), axis=1)\n",
        "print(metrics.classification_report(y_train_val, predictions, digits=10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0  0.0000000000 0.0000000000 0.0000000000       430\n",
            "           1  0.0000000000 0.0000000000 0.0000000000       442\n",
            "           2  0.0000000000 0.0000000000 0.0000000000       372\n",
            "           3  0.0000000000 0.0000000000 0.0000000000       421\n",
            "           4  0.0000000000 0.0000000000 0.0000000000       422\n",
            "           5  0.0835000000 1.0000000000 0.1541301338       334\n",
            "           6  0.0000000000 0.0000000000 0.0000000000       400\n",
            "           7  0.0000000000 0.0000000000 0.0000000000       401\n",
            "           8  0.0000000000 0.0000000000 0.0000000000       379\n",
            "           9  0.0000000000 0.0000000000 0.0000000000       399\n",
            "\n",
            "    accuracy                      0.0835000000      4000\n",
            "   macro avg  0.0083500000 0.1000000000 0.0154130134      4000\n",
            "weighted avg  0.0069722500 0.0835000000 0.0128698662      4000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vypm4hoPC_Au"
      },
      "source": [
        "input_shape = (28,28,1)\n",
        "input_layer = Input(input_shape)\n",
        "\n",
        "layer11 = Conv2D(16,(1,1),activation = tf.nn.relu, input_shape = input_shape)(input_layer)  #op 28*28\n",
        "layer12 = Conv2D(16,(3,3),activation = tf.nn.relu, input_shape = input_shape)(input_layer)  #op 26*26\n",
        "layer13 = Conv2D(16,(5,5),activation = tf.nn.relu, input_shape = input_shape)(input_layer)  #op 24*24\n",
        "layer14 = Conv2D(16,(7,7),activation = tf.nn.relu, input_shape = input_shape)(input_layer)  #op 22*22\n",
        "\n",
        "layer21 = MaxPool2D((2,2))(layer11)  #14\n",
        "layer22 = MaxPool2D((2,2))(layer12)  #13\n",
        "layer23 = MaxPool2D((2,2))(layer13)  #12\n",
        "layer24 = MaxPool2D((2,2))(layer14)  #11\n",
        "\n",
        "layer31 = Conv2D(32,(5,5),activation = tf.nn.relu)(layer21) #10\n",
        "layer32 = Conv2D(32,(3,3),activation = tf.nn.relu)(layer22) #11\n",
        "layer33 = Conv2D(32,(3,3),activation = tf.nn.relu)(layer23) #10\n",
        "layer34 = Conv2D(32,(1,1),activation = tf.nn.relu)(layer24) #11\n",
        "\n",
        "layer41 = MaxPool2D((2,2))(layer31)\n",
        "layer42 = MaxPool2D((2,2))(layer32)\n",
        "layer43 = MaxPool2D((2,2))(layer33)\n",
        "layer44 = MaxPool2D((2,2))(layer34)\n",
        "\n",
        "flatten51 = Flatten()(layer41)\n",
        "flatten52 = Flatten()(layer42)\n",
        "flatten53 = Flatten()(layer43)\n",
        "flatten54 = Flatten()(layer44)\n",
        "\n",
        "layer61 = Dense(128,activation = tf.nn.relu)(flatten51)\n",
        "layer62 = Dense(128,activation = tf.nn.relu)(flatten52)\n",
        "layer63 = Dense(128,activation = tf.nn.relu)(flatten53)\n",
        "layer64 = Dense(128,activation = tf.nn.relu)(flatten54)\n",
        "\n",
        "layer71 = tf.keras.layers.concatenate([layer61 , layer61 , layer63 , layer64],axis=1)\n",
        "\n",
        "output_layer = Dense(10, activation = tf.nn.softmax)(layer71)\n",
        "\n",
        "model = Model(input_layer,output_layer)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,optimizer='Adam', metrics = ['accuracy'])\n",
        "\n",
        "# layer = Conv2D(64,(3,3),activation = tf.nn.relu, padding='same')(layer)\n",
        "# layer = Conv2D(64,(3,3),activation = tf.nn.relu, padding='same')(layer)\n",
        "# layer = MaxPool2D((2,2))(layer)\n",
        "# layer = Conv2D(128,(3,3),activation = tf.nn.relu, padding='same')(layer)\n",
        "# layer = Conv2D(128,(3,3),activation = tf.nn.relu, padding='same')(layer)\n",
        "# layer = Conv2D(128,(3,3),activation = tf.nn.relu, padding='same')(layer)\n",
        "# layer = MaxPool2D((2,2))(layer)\n",
        "\n",
        "\n",
        "\n",
        "# flatten = Flatten()(layer)\n",
        "\n",
        "# layer = Dense(512,activation = tf.nn.relu)(flatten)\n",
        "# layer = Dropout(0.25)(layer)\n",
        "# layer = Dense(512,activation = tf.nn.relu)(layer)\n",
        "# layer  = Dropout(0.25)(layer)\n",
        "# layer = Dense(512,activation = tf.nn.relu)(layer)\n",
        "# layer  = Dropout(0.25)(layer)\n",
        "# output_layer = Dense(10, activation = tf.nn.softmax)(layer)\n",
        "# model = Model(input_layer,output_layer)\n",
        "\n",
        "# model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,optimizer='Adam', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m8_addKJrj-",
        "outputId": "e3441231-5a19-46dd-8111-cc6899a5bbfc"
      },
      "source": [
        "history = model.fit(generator_train.x, generator_train.y, epochs=100, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "594/594 [==============================] - 6s 8ms/step - loss: 1.5647 - accuracy: 0.9077\n",
            "Epoch 2/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4899 - accuracy: 0.9733\n",
            "Epoch 3/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4818 - accuracy: 0.9805\n",
            "Epoch 4/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4774 - accuracy: 0.9846\n",
            "Epoch 5/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4747 - accuracy: 0.9871\n",
            "Epoch 6/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4729 - accuracy: 0.9888\n",
            "Epoch 7/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4719 - accuracy: 0.9896\n",
            "Epoch 8/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4712 - accuracy: 0.9902\n",
            "Epoch 9/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4697 - accuracy: 0.9918\n",
            "Epoch 10/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4693 - accuracy: 0.9920\n",
            "Epoch 11/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4687 - accuracy: 0.9930\n",
            "Epoch 12/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4686 - accuracy: 0.9927\n",
            "Epoch 13/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4675 - accuracy: 0.9940\n",
            "Epoch 14/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4672 - accuracy: 0.9940\n",
            "Epoch 15/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4675 - accuracy: 0.9938\n",
            "Epoch 16/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4680 - accuracy: 0.9932\n",
            "Epoch 17/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4670 - accuracy: 0.9941\n",
            "Epoch 18/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4659 - accuracy: 0.9955\n",
            "Epoch 19/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4650 - accuracy: 0.9962\n",
            "Epoch 20/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4663 - accuracy: 0.9949\n",
            "Epoch 21/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4661 - accuracy: 0.9952\n",
            "Epoch 22/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4654 - accuracy: 0.9959\n",
            "Epoch 23/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4650 - accuracy: 0.9962\n",
            "Epoch 24/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4651 - accuracy: 0.9961\n",
            "Epoch 25/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4650 - accuracy: 0.9964\n",
            "Epoch 26/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4647 - accuracy: 0.9964\n",
            "Epoch 27/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4648 - accuracy: 0.9964\n",
            "Epoch 28/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9969\n",
            "Epoch 29/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9969\n",
            "Epoch 30/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4654 - accuracy: 0.9957\n",
            "Epoch 31/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4657 - accuracy: 0.9954\n",
            "Epoch 32/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9968\n",
            "Epoch 33/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4646 - accuracy: 0.9965\n",
            "Epoch 34/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4636 - accuracy: 0.9975\n",
            "Epoch 35/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4650 - accuracy: 0.9960\n",
            "Epoch 36/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4647 - accuracy: 0.9964\n",
            "Epoch 37/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4650 - accuracy: 0.9961\n",
            "Epoch 38/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4646 - accuracy: 0.9965\n",
            "Epoch 39/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4642 - accuracy: 0.9969\n",
            "Epoch 40/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9966\n",
            "Epoch 41/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9968\n",
            "Epoch 42/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4642 - accuracy: 0.9969\n",
            "Epoch 43/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4652 - accuracy: 0.9959\n",
            "Epoch 44/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9966\n",
            "Epoch 45/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4638 - accuracy: 0.9974\n",
            "Epoch 46/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4638 - accuracy: 0.9974\n",
            "Epoch 47/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9968\n",
            "Epoch 48/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4644 - accuracy: 0.9967\n",
            "Epoch 49/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4647 - accuracy: 0.9965\n",
            "Epoch 50/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4641 - accuracy: 0.9970\n",
            "Epoch 51/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4651 - accuracy: 0.9959\n",
            "Epoch 52/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4638 - accuracy: 0.9973\n",
            "Epoch 53/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4642 - accuracy: 0.9970\n",
            "Epoch 54/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4638 - accuracy: 0.9974\n",
            "Epoch 55/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4634 - accuracy: 0.9977\n",
            "Epoch 56/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4647 - accuracy: 0.9965\n",
            "Epoch 57/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4648 - accuracy: 0.9963\n",
            "Epoch 58/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9969\n",
            "Epoch 59/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4647 - accuracy: 0.9963\n",
            "Epoch 60/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9966\n",
            "Epoch 61/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4638 - accuracy: 0.9974\n",
            "Epoch 62/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4646 - accuracy: 0.9965\n",
            "Epoch 63/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9969\n",
            "Epoch 64/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4642 - accuracy: 0.9970\n",
            "Epoch 65/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4640 - accuracy: 0.9972\n",
            "Epoch 66/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4644 - accuracy: 0.9968\n",
            "Epoch 67/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4639 - accuracy: 0.9972\n",
            "Epoch 68/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4642 - accuracy: 0.9970\n",
            "Epoch 69/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9967\n",
            "Epoch 70/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4634 - accuracy: 0.9977\n",
            "Epoch 71/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4626 - accuracy: 0.9985\n",
            "Epoch 72/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4644 - accuracy: 0.9968\n",
            "Epoch 73/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4644 - accuracy: 0.9968\n",
            "Epoch 74/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9967\n",
            "Epoch 75/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4641 - accuracy: 0.9970\n",
            "Epoch 76/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4634 - accuracy: 0.9978\n",
            "Epoch 77/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4638 - accuracy: 0.9973\n",
            "Epoch 78/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9966\n",
            "Epoch 79/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4646 - accuracy: 0.9965\n",
            "Epoch 80/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4644 - accuracy: 0.9967\n",
            "Epoch 81/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4641 - accuracy: 0.9970\n",
            "Epoch 82/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4640 - accuracy: 0.9972\n",
            "Epoch 83/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4637 - accuracy: 0.9975\n",
            "Epoch 84/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4634 - accuracy: 0.9977\n",
            "Epoch 85/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4636 - accuracy: 0.9975\n",
            "Epoch 86/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4630 - accuracy: 0.9981\n",
            "Epoch 87/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4651 - accuracy: 0.9960\n",
            "Epoch 88/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4647 - accuracy: 0.9964\n",
            "Epoch 89/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4641 - accuracy: 0.9970\n",
            "Epoch 90/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4642 - accuracy: 0.9969\n",
            "Epoch 91/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4649 - accuracy: 0.9962\n",
            "Epoch 92/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4644 - accuracy: 0.9968\n",
            "Epoch 93/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4635 - accuracy: 0.9976\n",
            "Epoch 94/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9968\n",
            "Epoch 95/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4648 - accuracy: 0.9964\n",
            "Epoch 96/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4640 - accuracy: 0.9971\n",
            "Epoch 97/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4641 - accuracy: 0.9971\n",
            "Epoch 98/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4641 - accuracy: 0.9969\n",
            "Epoch 99/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4637 - accuracy: 0.9974\n",
            "Epoch 100/100\n",
            "594/594 [==============================] - 5s 8ms/step - loss: 1.4628 - accuracy: 0.9983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9Etmsv1J0Ei",
        "outputId": "fb778db2-e0a1-4444-b30d-06ffe66aee15"
      },
      "source": [
        "predictions = np.argmax(model.predict(X_train_val), axis=1)\n",
        "print(metrics.classification_report(y_train_val, predictions, digits=10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0  0.9907407407 0.9953488372 0.9930394432       430\n",
            "           1  0.9909502262 0.9909502262 0.9909502262       442\n",
            "           2  0.9761273210 0.9892473118 0.9826435247       372\n",
            "           3  0.9904761905 0.9881235154 0.9892984542       421\n",
            "           4  0.9836448598 0.9976303318 0.9905882353       422\n",
            "           5  0.9538904899 0.9910179641 0.9720998532       334\n",
            "           6  0.9875930521 0.9950000000 0.9912826899       400\n",
            "           7  0.9900497512 0.9925187032 0.9912826899       401\n",
            "           8  0.9918256131 0.9604221636 0.9758713137       379\n",
            "           9  1.0000000000 0.9573934837 0.9782330346       399\n",
            "\n",
            "    accuracy                      0.9860000000      4000\n",
            "   macro avg  0.9855298245 0.9857652537 0.9855289465      4000\n",
            "weighted avg  0.9861937478 0.9860000000 0.9859855576      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMXUhfvkPxMw"
      },
      "source": [
        "datagen.fit(X_train)\n",
        "generator_train  = datagen.flow(X_train,y_train,batch_size=256)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrosWOlFQzNc",
        "outputId": "f00481b4-7a69-41a1-dbcf-0dc27a16453c"
      },
      "source": [
        "history = model.fit(generator_train.x, generator_train.y, epochs=100, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "657/657 [==============================] - 7s 9ms/step - loss: 1.5838 - accuracy: 0.8874\n",
            "Epoch 2/100\n",
            "657/657 [==============================] - 6s 9ms/step - loss: 1.4872 - accuracy: 0.9758\n",
            "Epoch 3/100\n",
            "657/657 [==============================] - 6s 8ms/step - loss: 1.4797 - accuracy: 0.9825\n",
            "Epoch 4/100\n",
            "657/657 [==============================] - 6s 9ms/step - loss: 1.4761 - accuracy: 0.9861\n",
            "Epoch 5/100\n",
            "657/657 [==============================] - 6s 8ms/step - loss: 1.4736 - accuracy: 0.9883\n",
            "Epoch 6/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4724 - accuracy: 0.9892\n",
            "Epoch 7/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4705 - accuracy: 0.9910\n",
            "Epoch 8/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4708 - accuracy: 0.9907\n",
            "Epoch 9/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4702 - accuracy: 0.9911\n",
            "Epoch 10/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4700 - accuracy: 0.9914\n",
            "Epoch 11/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4689 - accuracy: 0.9925\n",
            "Epoch 12/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4681 - accuracy: 0.9933\n",
            "Epoch 13/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4681 - accuracy: 0.9931\n",
            "Epoch 14/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4672 - accuracy: 0.9941\n",
            "Epoch 15/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4671 - accuracy: 0.9941\n",
            "Epoch 16/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4675 - accuracy: 0.9937\n",
            "Epoch 17/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4670 - accuracy: 0.9943\n",
            "Epoch 18/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4662 - accuracy: 0.9951\n",
            "Epoch 19/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4664 - accuracy: 0.9948\n",
            "Epoch 20/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4658 - accuracy: 0.9954\n",
            "Epoch 21/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4655 - accuracy: 0.9956\n",
            "Epoch 22/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4655 - accuracy: 0.9957\n",
            "Epoch 23/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4653 - accuracy: 0.9958\n",
            "Epoch 24/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4666 - accuracy: 0.9945\n",
            "Epoch 25/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4660 - accuracy: 0.9952\n",
            "Epoch 26/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4658 - accuracy: 0.9954\n",
            "Epoch 27/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4651 - accuracy: 0.9960\n",
            "Epoch 28/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4653 - accuracy: 0.9959\n",
            "Epoch 29/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4650 - accuracy: 0.9962\n",
            "Epoch 30/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4650 - accuracy: 0.9961\n",
            "Epoch 31/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4649 - accuracy: 0.9962\n",
            "Epoch 32/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4642 - accuracy: 0.9969\n",
            "Epoch 33/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4652 - accuracy: 0.9960\n",
            "Epoch 34/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4655 - accuracy: 0.9957\n",
            "Epoch 35/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4647 - accuracy: 0.9964\n",
            "Epoch 36/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9969\n",
            "Epoch 37/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9969\n",
            "Epoch 38/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4651 - accuracy: 0.9961\n",
            "Epoch 39/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4654 - accuracy: 0.9956\n",
            "Epoch 40/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4638 - accuracy: 0.9974\n",
            "Epoch 41/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9966\n",
            "Epoch 42/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4651 - accuracy: 0.9961\n",
            "Epoch 43/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4649 - accuracy: 0.9963\n",
            "Epoch 44/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4642 - accuracy: 0.9970\n",
            "Epoch 45/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4646 - accuracy: 0.9965\n",
            "Epoch 46/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9966\n",
            "Epoch 47/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4642 - accuracy: 0.9969\n",
            "Epoch 48/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4639 - accuracy: 0.9973\n",
            "Epoch 49/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4642 - accuracy: 0.9970\n",
            "Epoch 50/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9968\n",
            "Epoch 51/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4646 - accuracy: 0.9965\n",
            "Epoch 52/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4652 - accuracy: 0.9960\n",
            "Epoch 53/100\n",
            "657/657 [==============================] - 6s 8ms/step - loss: 1.4648 - accuracy: 0.9963\n",
            "Epoch 54/100\n",
            "657/657 [==============================] - 6s 8ms/step - loss: 1.4654 - accuracy: 0.9957\n",
            "Epoch 55/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9969\n",
            "Epoch 56/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9969\n",
            "Epoch 57/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4647 - accuracy: 0.9964\n",
            "Epoch 58/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4635 - accuracy: 0.9977\n",
            "Epoch 59/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9967\n",
            "Epoch 60/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4639 - accuracy: 0.9973\n",
            "Epoch 61/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9967\n",
            "Epoch 62/100\n",
            "657/657 [==============================] - 6s 9ms/step - loss: 1.4638 - accuracy: 0.9974\n",
            "Epoch 63/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4639 - accuracy: 0.9973\n",
            "Epoch 64/100\n",
            "657/657 [==============================] - 6s 8ms/step - loss: 1.4635 - accuracy: 0.9976\n",
            "Epoch 65/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4658 - accuracy: 0.9953\n",
            "Epoch 66/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4646 - accuracy: 0.9965\n",
            "Epoch 67/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4641 - accuracy: 0.9970\n",
            "Epoch 68/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4649 - accuracy: 0.9962\n",
            "Epoch 69/100\n",
            "657/657 [==============================] - 6s 8ms/step - loss: 1.4637 - accuracy: 0.9975\n",
            "Epoch 70/100\n",
            "657/657 [==============================] - 6s 8ms/step - loss: 1.4642 - accuracy: 0.9969\n",
            "Epoch 71/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4639 - accuracy: 0.9972\n",
            "Epoch 72/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4646 - accuracy: 0.9966\n",
            "Epoch 73/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9966\n",
            "Epoch 74/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4649 - accuracy: 0.9963\n",
            "Epoch 75/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4641 - accuracy: 0.9971\n",
            "Epoch 76/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4657 - accuracy: 0.9954\n",
            "Epoch 77/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4652 - accuracy: 0.9960\n",
            "Epoch 78/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9966\n",
            "Epoch 79/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4637 - accuracy: 0.9974\n",
            "Epoch 80/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4651 - accuracy: 0.9960\n",
            "Epoch 81/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4642 - accuracy: 0.9970\n",
            "Epoch 82/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4644 - accuracy: 0.9967\n",
            "Epoch 83/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4644 - accuracy: 0.9968\n",
            "Epoch 84/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4651 - accuracy: 0.9960\n",
            "Epoch 85/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4640 - accuracy: 0.9971\n",
            "Epoch 86/100\n",
            "657/657 [==============================] - 6s 8ms/step - loss: 1.4641 - accuracy: 0.9970\n",
            "Epoch 87/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4646 - accuracy: 0.9965\n",
            "Epoch 88/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4644 - accuracy: 0.9967\n",
            "Epoch 89/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4636 - accuracy: 0.9975\n",
            "Epoch 90/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4635 - accuracy: 0.9977\n",
            "Epoch 91/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4638 - accuracy: 0.9974\n",
            "Epoch 92/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4646 - accuracy: 0.9965\n",
            "Epoch 93/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4645 - accuracy: 0.9966\n",
            "Epoch 94/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4649 - accuracy: 0.9962\n",
            "Epoch 95/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4648 - accuracy: 0.9963\n",
            "Epoch 96/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4644 - accuracy: 0.9967\n",
            "Epoch 97/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4635 - accuracy: 0.9976\n",
            "Epoch 98/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4650 - accuracy: 0.9962\n",
            "Epoch 99/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4644 - accuracy: 0.9967\n",
            "Epoch 100/100\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4643 - accuracy: 0.9969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KuUN0qzQ7FA"
      },
      "source": [
        "predictions = np.argmax(model.predict(X_test), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryIdWhUtTjwQ",
        "outputId": "3876ad90-d87c-475a-9f49-6a7ad5a1a71c"
      },
      "source": [
        "len(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28000"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6WWG6MSTHqt",
        "outputId": "27ed27ea-5fa3-49ca-8164-fb4dd6535be6"
      },
      "source": [
        "predictions[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 9, 0, 3, 7, 0, 3, 0, 3, 5, 7, 4, 0, 4, 3, 3, 1, 9, 0, 9, 1,\n",
              "       1, 5, 7, 4, 2, 7, 4, 7, 7, 5, 4, 2, 6, 2, 5, 5, 1, 6, 7, 7, 4, 9,\n",
              "       8, 7, 8, 2, 6, 7, 6, 8, 8, 3, 8, 2, 1, 2, 2, 0, 4, 1, 7, 0, 0, 0,\n",
              "       1, 9, 0, 1, 6, 5, 8, 8, 2, 8, 9, 9, 2, 3, 5, 4, 1, 0, 9, 2, 4, 3,\n",
              "       6, 7, 2, 0, 6, 6, 1, 4, 3, 9, 7, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aTSRUANTr7X",
        "outputId": "ea8c1dfc-3cd5-4406-cc1a-28c40bdbc915"
      },
      "source": [
        "[i for i in range(1,10 + 1)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HIt_tB6Rl0d"
      },
      "source": [
        "import pandas as pd\n",
        "opdf = pd.DataFrame() \n",
        "opdf[\"ImageId\"] = [i for i in range(1 , len(predictions)+1 )]\n",
        "opdf[\"Label\"] = predictions\n",
        "opdf.to_csv(r\"/content/drive/MyDrive/digit_kaggle/\" + \"submission1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbjj12RUOCyK"
      },
      "source": [
        "#model_hist = model.fit(generator_train,validation_data=(X_train_val , y_train_val),epochs=100,verbose=1,batch_size=32)  #this works fine just not using for now"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}